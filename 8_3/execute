import csv
import argparse
import json
import io
import os
from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')

list_to_csv = []

"""
dem_path = 'C:\\Users\\parth\\PycharmProjects\\bipartisanship\\dem_files\\'
dem_files = os.listdir(dem_path)

rep_path = 'C:\\Users\\parth\\PycharmProjects\\bipartisanship\\rep_files\\'
rep_files = os.listdir(rep_path)

dem_count_by_id = {}
rep_count_by_id = {}

file_end_num = ''

for filename in dem_files:
  with open(dem_path + filename, 'r') as dem_file:
    followers = dem_file.read().split(',')
    for follower in followers:
      dem_count_by_id[follower] = dem_count_by_id[follower] + 1 if follower in dem_count_by_id else 1

for filename in rep_files:
  with open(rep_path + filename, 'r') as rep_file:
    followers = rep_file.read().split(',')
    for follower in followers:
      rep_count_by_id[follower] = rep_count_by_id[follower] + 1 if follower in rep_count_by_id else 1
"""

def get_nlp_info(tweet):
        #entities, parts of speech
        result = nlp.annotate(tweet['text'],
                              properties={
                                  'annotators': 'sentiment, ner, pos',
                                  'outputFormat': 'json',
                                  'timeout': 1000,
                              })

        sentiments = []

        pos = []
        s_exp = []

        index = 0

        if 'sentences' in result:
            for s in result['sentences']:
                for t in s['tokens']:
                    pos.append(t['pos'])
                s_exp.append(s['sentimentTree'])

        tokens = []

        sentiments = []

        s_index = 0

        for s in result['sentences']:
            sentiments.append(int(s['sentimentValue']))
            temp_list = []
            for n in s['tokens']:
                temp_list.append(n)
            tokens.append(temp_list)

        entities = []
        entity_indexes = []
        entity_sentiments = []

        last_ner = 'O'
        current_entity = None

        s_index = -1

        for s in tokens:
            s_index += 1
            for token in s:
                ner = token['ner']
                if ner != last_ner:
                    if current_entity is not None:
                        entities.append(current_entity)
                        entity_indexes.append(s_index)
                    if ner == 'O':
                        current_entity = None
                    else:
                        current_entity = token['word']
                elif ner == last_ner:
                    if current_entity is not None:
                        current_entity += ' ' + token['word']
                last_ner = ner

        for i in entity_indexes:
            entity_sentiments.append(sentiments[i])

        return entities, pos, s_exp, entity_sentiments


def get_text(tweet):
    if 'text' in tweet:
        raw_text = tweet['text']
        words = raw_text.split()
        punctuation = "!#$%&()*+,./:;<=>?@[\]^_`{|}~"
        table = str.maketrans('', '', punctuation)
        tokenized_text = [w.translate(table) for w in words]

        text = raw_text.encode(encoding = "UTF-8", errors = "ignore")
    return raw_text, tokenized_text

"""
def get_party(tweet):
    id = str(tweet["user"]["id"])
    dem_count = 0
    rep_count = 0

    if id in dem_count_by_id:
        dem_count = dem_count_by_id.get(id)

    if id in dem_count_by_id:
        rep_count = rep_count_by_id.get(id)
        
    party = ''

    if dem_count > rep_count:
        party = "democrat"
    elif rep_count > dem_count:
        party = "republican"
    else:
        party = "neutral"

    return party
"""

def append_info(tweet):
    #Entity, Text, Tokenized Text, User ID, Following, Creation Date, Political Party, Parts Of Speech, Sentiment Tree S-expression
    addition_list = []

    entities, pos, s_exp, entity_sentiments = get_nlp_info(tweet)
    text, tokenized_text = get_text(tweet)

    addition_list.append(text)
    addition_list.append(tokenized_text)
    addition_list.append(tweet["user"]["id"])
    addition_list.append(tweet["user"]["friends_count"])
    addition_list.append(tweet["created_at"])
    #addition_list.append(get_party(tweet))
    addition_list.append(pos)
    addition_list.append(s_exp)

    final_list = []

    length = len(addition_list)

    for i in range(len(entities)):
        temp_list = addition_list[-length:]
        temp_list.insert(0, entities[i])
        temp_list.insert(1, entity_sentiments[i])
        final_list.append(temp_list)

    for i in range(len(entities)):
        list_to_csv.append(final_list[i])

def parse_args(): #via Gabor
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input', dest='input', type=str, help='The JSONL file with our tweets')
    parser.add_argument('-o', '--output', dest='output', type=str, help="The CSV we're outputting annotated tweets to")
    return parser.parse_args()


def read_jsonl(path): #via Gabor
    tweets = []
    with open(path, 'r') as jsonl_file:
        lines = jsonl_file.readlines()  # list of strings
        for line in lines:
            if line.strip() != '':
                tweet = json.loads(line)
                if 'delete' not in tweet:
                    tweets.append(tweet)
    return tweets

def write_tsv(path, contents): #via Gabor
    with io.open(path, 'w', encoding = 'utf-8') as tsv_file:
        writer = csv.writer(tsv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        for line in contents:
            writer.writerow(line)


def main(): #via Gabor
    args = parse_args()
    tweets = read_jsonl(args.input)
    for tweet in tweets:
        append_info(tweet)
    write_tsv(args.output, list_to_csv)


if __name__ == '__main__': #via Gabor
    # We don't want to run code if we're importing this as a library. So, this block
    # will only run if this is the main file that we're running from
    main()
